[
  {
    "slug": "monkeys-librarian-magician-llm-scaling-agi",
    "title": "The Monkeys, the Librarian, and the Magician: Why LLMs aren't an absolute path to AGI",
    "date": "2025-12-07",
    "author": "Brian Fending",
    "excerpt": "The hockey stick curves toward AGI that populate pitch decks assume scaling LLMs will yield qualitatively different results. Three metaphors illuminate the structural limits: the verification wall, the interpolation ceiling, and the perception gap between demos and production.",
    "content": "The hockey stick curves toward AGI that populate pitch decks assume scaling LLMs will yield qualitatively different results. But those impressive scaling curves use logarithmic scales that make diminishing returns look like steady progress. DeepMind's Chinchilla study found a power law relationship where the loss function flattens and more compute yields less improvement.\n\nThe counterargument invoking agentic systems, multi-agent architectures, and world models concedes the core claim. You're no longer arguing that scaling LLMs is the path, but that LLMs might be one component of a much more complex system. That's a different bet with different timelines and capital requirements.\n\nExecutives drawing hockey stick curves to AGI solely through LLM scaling are betting on a trajectory that bends in ways the underlying technology can't support. If AGI emerges, it won't be because we scaled token prediction harder, but because we built fundamentally different coordination, reasoning, and embodiment layers on top of it.",
    "tags": [
      "AI",
      "AGI",
      "LLM scaling",
      "machine learning",
      "AI limitations",
      "enterprise AI",
      "risk management"
    ],
    "featuredImage": "monkeys_librarian_magician.png",
    "metaDescription": "The hockey stick curves toward AGI that populate pitch decks assume scaling LLMs will yield qualitatively different results. Three metaphors illuminate the structural limits: the verification wall, the interpolation ceiling, and the perception gap between demos and production.",
    "linkedinUrl": "https://www.linkedin.com/pulse/monkeys-librarian-magician-why-llms-arent-absolute-path-brian-fending-ljqte",
    "substackUrl": "https://open.substack.com/pub/brianfending/p/monkeys-librarian-magician-llm-scaling-agi"
  },
  {
    "slug": "shadow-ai-discovery-containment-practitioners-guide",
    "title": "From Shadow IT to Shadow AI: A Practitioner's Guide to Discovery and Containment",
    "date": "2025-11-16",
    "author": "Brian Fending",
    "excerpt": "Shadow AI represents a fundamental shift in how unauthorized technology enters the enterprise. Where shadow IT required deliberate procurement decisions, shadow AI often arrives embedded in existing approved platforms, creating governance challenges that demand new approaches to discovery and containment.",
    "content": "Shadow AI represents a fundamental shift in how unauthorized technology enters the enterprise. Where shadow IT required deliberate procurement decisions, shadow AI often arrives embedded in existing approved platforms. The technology stack expands without a single new purchase order.\n\nRecent ISACA research confirms 83% of IT professionals believe employees are using AI, yet only 31% of organizations have implemented formal AI policies. Organizations discover they're running three separate AI copywriting platforms in marketing alone, while enterprise pilots go underutilized.\n\nAddressing shadow AI requires adapting traditional governance approaches. Discovery demands expense pattern analysis, SaaS feature inventory, and usage pattern monitoring. Not all shadow AI presents equivalent risk - classification must consider technology characteristics and data environment to enable effective prioritization. The organizations seeing the least shadow AI aren't those with the most restrictive policies, they're the ones with the fastest approved alternative delivery.",
    "tags": [
      "Shadow AI",
      "AI governance",
      "risk management",
      "shadow IT",
      "compliance",
      "enterprise AI",
      "security"
    ],
    "featuredImage": "shadow_ai.png",
    "metaDescription": "Shadow AI represents a fundamental shift in how unauthorized technology enters the enterprise. Where shadow IT required deliberate procurement decisions, shadow AI often arrives embedded in existing approved platforms, creating governance challenges that demand new approaches to discovery and containment.",
    "linkedinUrl": "https://www.linkedin.com/pulse/from-shadow-ai-practitioners-guide-discovery-brian-fending-mhzje",
    "substackUrl": "https://brianfending.substack.com/p/shadow-ai-discovery-containment-practitioners-guide"
  },
  {
    "slug": "mcp-security-problem-supply-chain",
    "title": "The MCP Security Problem",
    "date": "2025-10-27",
    "author": "Brian Fending",
    "excerpt": "Recent research exposed over 3,000 MCP servers through a single path traversal vulnerability in centralized infrastructure. Every AI integration creates security debt we have few ways to track, and most organizations are flying blind on what's actually running.",
    "content": "Recently published research revealed findings on MCP server vulnerabilities that exposed over 3,000 servers and compromised API keys from thousands of clients. The broader issue is that every AI integration creates security debt we have few ways to track. The research demonstrates how traditional security approaches fail for AI integrations that expose functionality to a much broader user base through conversational interfaces.\n\nThe right tooling surfaces misconfigurations immediately, scores them by risk, and gives teams actionable data instead of hoping someone notices during the next pen test. The MCP security problem isn't theoretical anymore - we need to figure out how to secure these integrations at scale.",
    "tags": [
      "MCP",
      "security",
      "supply chain",
      "AI integrations",
      "vulnerability research",
      "threat intelligence",
      "risk management"
    ],
    "featuredImage": "mcp_server_security_problem.png",
    "metaDescription": "Recent research exposed over 3,000 MCP servers through a single path traversal vulnerability in centralized infrastructure. Every AI integration creates security debt we have few ways to track, and most organizations are flying blind on what's actually running.",
    "linkedinUrl": "https://www.linkedin.com/pulse/mcp-security-problem-brian-fending-2mhwc",
    "substackUrl": "https://brianfending.substack.com/p/mcp-security-problem-supply-chain"
  },
  {
    "slug": "blueprint-rebuilding-consulting-practice-around-assessments",
    "title": "A Blueprint for Rebuilding Your Consulting Practice Around Assessments",
    "date": "2025-09-16",
    "author": "Brian Fending",
    "excerpt": "Rebuilding MADE, Inc. for the Age of AI: A blueprint for transforming a dormant consulting practice into an AI-powered assessment platform that demonstrates methodology through action, showing how modern development enables sophisticated consulting platforms.",
    "content": "Most consulting websites describe what you can do without demonstrating how you think and execute. After six years, I found MADE, Inc. sitting as abandoned WordPress brochureware. The rebuild transformed it into an AI-powered assessment platform that makes prospects experience methodology before ever talking to me.\n\nThe platform features four assessment tracks addressing consistent organizational struggles: AI Risk Assessment revealing the 3-5x hidden AI applications companies don't realize they're running, Product Development Maturity distinguishing between having a process and having a scaling process, Technology Governance separating theater from enablement, and Security Posture assessing pattern anticipation beyond checklists. Each assessment uses questions based on respected research, evolving from surface-level checkboxes to reveal actual business problems and organizational maturity patterns.\n\nAI processes assessment data with extensive context about what scores mean in practice, generating nuanced insights about organizational dynamics that procedural logic couldn't capture. The platform stores and versions every summary tied to specific assessments, enabling historical consistency while allowing methodology evolution. This creates a foundation for proprietary market research as data accrues over time.",
    "tags": [
      "AI development",
      "consulting",
      "platform engineering",
      "Next.js",
      "WordPress migration",
      "assessment platforms",
      "Claude Code",
      "business transformation"
    ],
    "featuredImage": "madeinc_licensed_artwork_pixelated.png",
    "metaDescription": "A blueprint for rebuilding your consulting practice around assessments: Learn how I rebuilt MADE, Inc. for the Age of AI using Claude Code, Next.js, and AI-powered development practices. A case study in demonstrating consulting expertise through technology.",
    "linkedinUrl": "https://www.linkedin.com/pulse/turning-digital-dust-consulting-platform-rebuild-made-brian-fending-euafe",
    "substackUrl": "https://brianfending.substack.com/p/blueprint-rebuilding-consulting-practice-around-assessments"
  },
  {
    "slug": "implementing-nist-ai-rmf-managing-part-4-4",
    "title": "Implementing NIST AI RMF: Managing (Part 4 of 4)",
    "date": "2025-08-02",
    "author": "Brian Fending",
    "excerpt": "Operationalizing governance at scale: moving from pilot purgatory to production deployment while maintaining control. Part 4 transforms NIST frameworks into sustainable operations that deliver consistent business value without collapsing under operational complexity.",
    "content": "Most organizations get stuck between successful AI pilots and production deployment. Governance frameworks that work perfectly for a few pilot projects collapse under the operational complexity of managing dozens of AI systems in production. Teams build comprehensive frameworks but discover their approaches require manual oversight that doesn't scale.\n\nThe NIST AI RMF MANAGE function addresses this through four operational capabilities: risk prioritization integrated with existing systems, benefit maximization strategies focused on business outcomes, systematic third-party vendor management, and continuous monitoring with automated exception handling.\n\nTraditional checklist governance proves insufficient for emerging complexity like multi-agent systems and Model Context Protocol architectures. These create new blind spots through cascading hallucinations and autonomous behaviors that conflict with human oversight requirements. Organizations need \"AgentOps\" approaches that monitor agent behavior and maintain accountability across distributed AI systems.",
    "tags": [
      "NIST",
      "AI governance",
      "operational scale",
      "AI RMF",
      "vendor management",
      "enterprise AI",
      "risk management"
    ],
    "featuredImage": "NIST_AI_RMF_4_manage.png",
    "metaDescription": "Operationalizing governance at scale: moving from pilot purgatory to production deployment while maintaining control. Part 4 transforms NIST frameworks into sustainable operations that deliver consistent business value without collapsing under operational complexity.",
    "linkedinUrl": "https://www.linkedin.com/pulse/implementing-nist-ai-rmf-managing-part-4-brian-fending-hvexe",
    "substackUrl": "https://brianfending.substack.com/p/implementing-nist-ai-rmf-managing"
  },
  {
    "slug": "implementing-nist-ai-rmf-measuring-part-3-4",
    "title": "Implementing NIST AI RMF: Measuring (Part 3 of 4)",
    "date": "2025-07-25",
    "author": "Brian Fending",
    "excerpt": "Beyond trust theater: implementing metrics that actually matter for AI trustworthiness. Part 3 transforms measurement from technical performance dashboards to systematic evaluation of the seven NIST characteristics that determine whether AI systems are safe to deploy.",
    "content": "Most AI measurement systems focus on technical performance while missing basic safety and fairness evaluations. While 75% of AI initiatives fail, organizations continue reporting success based on narrow performance metrics alone, like tracking uptime while ignoring response quality.\n\nAI systems require evaluation across seven NIST trustworthiness characteristics that traditional metrics miss completely: validity, safety, security, accountability, explainability, privacy enhancement, and fairness. Organizations that evaluate trustworthiness before deployment avoid the drift and inconsistencies that destroy user trust.\n\nThe NIST AI RMF MEASURE function transforms measurement from performance theater into systematic trustworthiness evaluation. Instead of beautiful dashboards showing perfect technical metrics, organizations build frameworks that assess whether AI systems actually work safely in production.\n\nThis is Part 3 of the four-part NIST AI RMF implementation series. The part where you stop relying on convenience metrics, and start measuring trust (and trustworthiness!) before your AI systems fail in ways that matter.",
    "tags": [
      "NIST",
      "AI measurement",
      "trustworthiness metrics",
      "AI RMF",
      "AI evaluation",
      "enterprise AI",
      "risk management"
    ],
    "featuredImage": "NIST_AI_RMF_3_measure.png",
    "metaDescription": "Beyond trust theater: implementing metrics that actually matter for AI trustworthiness. Part 3 transforms measurement from technical performance dashboards to systematic evaluation of the seven NIST characteristics that determine whether AI systems are safe to deploy.",
    "linkedinUrl": "https://www.linkedin.com/pulse/implementing-nist-ai-rmf-measuring-part-3-4-brian-fending-1iige",
    "substackUrl": "https://brianfending.substack.com/p/implementing-nist-ai-rmf-measuring"
  },
  {
    "slug": "implementing-nist-ai-rmf-mapping-part-2-4",
    "title": "Implementing NIST AI RMF: Mapping (Part 2 of 4)",
    "date": "2025-06-30",
    "author": "Brian Fending",
    "excerpt": "The real AI governance crisis isn't the models you've formally approved, it's the ones you don't know exist. Part 2 tackles the visibility gap that's creating compliance exposure and security risks.",
    "content": "The real AI governance crisis isn't the models you've formally approved, it's the ones you don't know exist. Organizations consistently underestimate their AI footprint by 3-5x when conducting systematic inventories, creating immediate compliance exposure and security risks.\n\nTraditional IT asset management fails catastrophically for AI because vendors don't clearly disclose AI features. What gets sold as \"enhanced analytics\" or \"intelligent automation\" often includes machine learning models processing your data in ways that weren't part of the original contract review.\n\nShadow AI adoption compounds the problem. Marketing subscribes to AI writing platforms, sales deploys conversation intelligence software, finance adopts AI-powered forecasting tools, all without formal approval processes. Meanwhile, vendor updates quietly add AI functionality to existing systems.\n\nThe NIST MAP function transforms this chaos into strategic visibility through systematic discovery, categorization, and impact assessment. Organizations move from compliance theater to actionable intelligence about their actual AI landscape.\n\nThis is Part 2 of the four-part NIST AI RMF implementation series. Stop discovering AI applications during compliance audits. Start discovering them before they become problems.",
    "tags": [
      "NIST",
      "AI mapping",
      "AI discovery",
      "risk management",
      "AI RMF",
      "enterprise AI",
      "AI inventory"
    ],
    "featuredImage": "NIST_AI_RMF_2_map.png",
    "metaDescription": "The real AI governance crisis isn't the models you've formally approved, it's the ones you don't know exist. Part 2 tackles the visibility gap that's creating compliance exposure and security risks.",
    "linkedinUrl": "https://www.linkedin.com/pulse/implementing-nist-ai-rmf-mapping-part-2-4-brian-fending-dlt4e/",
    "substackUrl": "https://brianfending.substack.com/p/implementing-nist-ai-rmf-mapping"
  },
  {
    "slug": "implementing-nist-ai-rmf-governing-part-1-4",
    "title": "Implementing NIST AI RMF: Governing (Part 1 of 4)",
    "date": "2025-06-24",
    "author": "Brian Fending",
    "excerpt": "Transforming the NIST AI Risk Management Framework from compliance theater to strategic enablement. Part 1 focuses on governance structures that accelerate AI adoption while managing real risks.",
    "content": "Most AI initiatives fail because of governance problems, not technology problems. While 75% of AI projects don't deliver expected ROI, organizations keep burning budgets on \"governance theater\", or elaborate processes that miss the actual risks killing their projects.\n\nTraditional IT governance frameworks almost completely fail when it comes to AI. Your standard security reviews can't totally handle AI systems that need access to data sets no regular application would touch. Meanwhile, every software vendor is slapping AI features into their products without clear documentation or, sometimes, notice.\n\nThe NIST AI Risk Management Framework offers the first practical approach designed specifically for AI's unique challenges. Unlike compliance-heavy alternatives, it builds on existing structures and focuses on business enablement over process documentation.\n\nThis is Part 1 of a four-part series transforming NIST's framework into actionable implementation strategies. We'll cover how to build governance that accelerates AI adoption while managing real risks—moving from pilot purgatory to production value.",
    "tags": [
      "NIST",
      "AI governance",
      "risk management",
      "AI RMF",
      "compliance",
      "enterprise AI"
    ],
    "featuredImage": "NIST_AI_RMF_1_govern.png",
    "metaDescription": "Transforming the NIST AI Risk Management Framework from compliance theater to strategic enablement. Part 1 focuses on governance structures that accelerate AI adoption while managing real risks.",
    "linkedinUrl": "https://www.linkedin.com/pulse/implementing-nist-ai-rmf-governing-part-1-4-brian-fending-3wxse/",
    "substackUrl": "https://brianfending.substack.com/p/implementing-nist-ai-rmf-governing"
  },
  {
    "slug": "rethinking-team-topologies-ai-augmented-development",
    "title": "Rethinking Team Topologies for AI-Augmented Development",
    "date": "2025-06-17",
    "author": "Brian Fending",
    "excerpt": "Exploring how AI integration is reshaping software development team structures and the evolution from traditional PRD workflows to AI-augmented collaboration patterns.",
    "content": "Software development team structures are undergoing their most significant transformation since the advent of agile methodologies. When a single developer with AI assistance can produce what previously required an entire squad, traditional team topologies need fundamental rethinking.\n\nTwo distinct approaches to AI-augmented development are emerging: a single file approach that centralizes all context in one discoverable location that plays nice with autonomous AI agents, and the context directory approach that distributes knowledge across specialized documents. Each serves different organizational needs and project complexities.\n\nBut the real revolution isn't just about organizing documentation, it's about breaking down the traditional \"PRD wall\" between Product Vision and development implementation. Both approaches enable something unprecedented: embedding Product Vision directly into development context, allowing AI agents and developers to validate against original user needs rather than translated requirements. The choice between approaches depends on whether you're optimizing for immediate AI effectiveness or long-term organizational sophistication. The most successful teams are finding hybrid strategies that serve both autonomous AI agents and complex human collaboration needs.\n\nUnderstanding these emerging patterns will help you to better position your organization for the fundamental shift in how Product Vision flows through development teams.",
    "tags": [
      "team topologies",
      "AI",
      "software development",
      "organizational design",
      "DevOps"
    ],
    "featuredImage": "rethinking-team-topologies-ai-augmented-development.png",
    "metaDescription": "Exploring how AI integration is reshaping software development team structures and the evolution from traditional PRD workflows to AI-augmented collaboration patterns.",
    "linkedinUrl": "https://www.linkedin.com/pulse/rethinking-team-topologies-ai-augmented-development-brian-fending-teuke",
    "substackUrl": "https://brianfending.substack.com/p/rethinking-team-topologies-for-ai"
  },
  {
    "slug": "governance-gap-top-down-risk-management",
    "title": "The Governance Gap: Why Top-Down Risk Management is Critical",
    "date": "2025-06-06",
    "author": "Brian Fending",
    "excerpt": "McKinsey's latest research reveals a striking gap between GRC aspiration and implementation reality, further making the case for top-down approaches to risk management.",
    "content": "The digital transformation acceleration we've witnessed since 2022 has amplified the need for robust governance, risk, and compliance capabilities. Yet McKinsey's latest research reveals a substantial gap between GRC aspiration and implementation reality.\n\nMcKinsey's 2025 Global GRC Benchmarking Survey found that \"excellent governance, risk, and compliance (GRC) is a common aspiration, but how often is it a reality? For most companies, GRC is a work in progress.\" Despite 93% of organizations having framework documents, implementation gaps are enormous—nearly half lack formal governance procedures.\n\nThe survey reveals a striking correlation: organizations where the head of risk is positioned more than one level below the CEO report significantly less mature risk functions. This validates what ISACA has long advocated—that top-down approaches yield better results than bottom-up initiatives.\n\nPerhaps most concerning is the resource reality: 66% of companies operate risk management with just 20 or fewer full-time staff. When resources are this constrained, organizations can't afford ineffective approaches.\n\nThis is precisely why I developed the Matrix Approach to incremental DRP and BCP review—a multi-dimensional framework that addresses the exact challenges identified in McKinsey's research through classification systems, incremental review cadences, and progressive live drills.\n\nSo we don't really have a framework problem, we have an adoption problem. The research is clear: pragmatic-to-a-fault bottom-up approaches fail to deliver needed maturity. It's time to embrace top-down, matrix-based frameworks that connect executive priorities to operational activities.",
    "tags": [
      "risk management",
      "GRC",
      "governance",
      "compliance",
      "enterprise security"
    ],
    "featuredImage": "grc_topdown_article_art_2025-06-06_vignette.png",
    "metaDescription": "McKinsey's latest research reveals a striking gap between GRC aspiration and implementation reality, further making the case for top-down approaches to risk management.",
    "linkedinUrl": "https://www.linkedin.com/pulse/governance-gap-why-top-down-risk-management-critical-brian-fending-c2imc/",
    "substackUrl": "https://brianfending.substack.com/p/the-governance-gap-why-top-down-risk"
  },
  {
    "slug": "matrix-approach-incremental-drp-bcp-review",
    "title": "The Matrix Approach to Incremental DRP and BCP Review",
    "date": "2025-05-24",
    "author": "Brian Fending",
    "excerpt": "A multi-dimensional framework for maintaining disaster recovery and business continuity plans through incremental reviews, addressing the gap between documentation and actual recovery capabilities.",
    "content": "The technology landscape changes faster than ever, yet many organizations treat disaster recovery and business continuity planning as periodic \"check-the-box\" exercises rather than living documents. This creates a dangerous gap between documented plans and actual recovery capabilities.\n\nTraditional approaches suffer from annual review syndrome, documentation without testing, siloed planning, and outdated assumptions. The Matrix Approach offers a multi-dimensional framework that classifies every recovery component across three dimensions: System Tiers (0-3 based on criticality), Process Categories (A-D based on business impact), and Personnel Functions (Essential through Deferred).\n\nEach cell in the matrix gets its own review cadence, validation method, and ownership assignment. Monthly reviews focus on Tier 0 systems, quarterly reviews cover Tier 1 systems and Category A processes, while semi-annual and annual reviews encompass broader system validation.\n\nKey innovations include recovery component heat mapping, trigger-based reviews responding to organizational changes, composite recovery teams organized by capability rather than department, and clear component dependency mapping. Breaking monolithic plans into sub-plans with dedicated ownership ensures effective maintenance.\n\nCompared to traditional ISACA/COBIT approaches, the Matrix Approach provides better integration through its multi-dimensional view, distributes testing burden throughout the year, and creates a dynamic management system that responds to organizational changes.",
    "tags": [
      "disaster recovery",
      "business continuity",
      "risk management",
      "framework",
      "matrix approach"
    ],
    "featuredImage": "matrix-approach-drp-bcp.png",
    "metaDescription": "A multi-dimensional framework for maintaining disaster recovery and business continuity plans through incremental reviews, addressing the gap between documentation and actual recovery capabilities.",
    "linkedinUrl": "https://www.linkedin.com/pulse/matrix-approach-incremental-drp-bcp-review-brian-fending-3klie/",
    "substackUrl": "https://brianfending.substack.com/p/the-matrix-approach-to-incremental"
  },
  {
    "slug": "risk-management-ai",
    "title": "A Risk Management Analysis of Google's A2A and Anthropic's MCP",
    "date": "2025-04-10",
    "author": "Brian Fending",
    "excerpt": "Examining how traditional risk frameworks apply to emerging AI technologies, with a focus on agent-to-agent communication systems and multi-context planning.",
    "content": "Have you noticed it's been hard to keep pace with AI news lately? In this article, \nI examine how traditional risk frameworks apply to emerging AI technologies, with a particular \nfocus on Google's Agent-to-Agent (A2A) communication and Anthropic's Multi-Context Planning (MCP).\n\nThis piece explores:\n\n- The n^a potential workflows created by Agent-to-Agent communication\n- How traditional GRC frameworks fall short with modern AI implementations\n- Three practical risk mitigation strategies for enterprise AI deployment\n\nIf you're responsible for technology risk management in your organization, this analysis \nprovides a practical framework for approaching AI integration with appropriate safeguards.\n\nAs AI systems become more autonomous and capable of communicating with each other, the \ncomplexity of potential interactions grows exponentially. Enterprise risk teams need to \ndevelop new approaches to manage these interactions, particularly in regulated industries.\n\nTraditional governance, risk, and compliance (GRC) frameworks were designed for human-to-human \nand human-to-machine interactions. They struggle to account for complex machine-to-machine \ninteractions, especially when those machines possess adaptive learning capabilities.\n\nRead the full article to learn about specific frameworks and approaches that work \nin today's rapidly evolving AI landscape.",
    "tags": [
      "AI",
      "risk management",
      "technology"
    ],
    "featuredImage": "a2a_mcp_image.png",
    "metaDescription": "Examining how traditional risk frameworks apply to emerging AI technologies, with a focus on agent-to-agent communication systems and multi-context planning.",
    "linkedinUrl": "https://www.linkedin.com/pulse/risk-management-analysis-googles-a2a-anthropics-mcp-brian-fending-9yjhe/",
    "substackUrl": "https://brianfending.substack.com/p/a-risk-management-analysis-of-googles"
  }
]